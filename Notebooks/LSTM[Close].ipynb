{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM[Close].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DBIh2Z7121E"
      },
      "source": [
        "In this notebook we'll implement Deep Learning based LSTM learning model for predcting stock's closing prices .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnEzmW4N1x22",
        "outputId": "a30fc0fb-3779-49e4-ebeb-14efaf58cb39"
      },
      "source": [
        "!pip install yfinance "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 17.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.8.0 requests-2.27.1 yfinance-0.1.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUCNuPEBZSZy"
      },
      "source": [
        "#Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For reading stock data from yahoo\n",
        "from pandas_datareader.data import DataReader\n",
        "\n",
        "# For time stamps\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Collection::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "def collectData(ticker):\n",
        "  dataset = yf.download(ticker, start ='2015-01-01', end='2021-12-31', threads=True, progress = False)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "p322xxa-PBOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DataPreprocessing(dataset):\n",
        "  #Data Pre-processing:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "  \n",
        "  dataset = dataset.drop(columns = 'Adj Close', axis = 1)\n",
        "\n",
        "  #Deriving new features : \n",
        "  dataset['Range'] = dataset['High']-dataset['Low']\n",
        "  dataset['a/d'] = ((dataset['Close']-dataset['Low'])*(dataset['High']-dataset['Close']))/(dataset['High']-dataset['Low'])\n",
        "  dataset['15MA'] = dataset['Close'].rolling(window = 15).mean()\n",
        "  dataset['15EMA'] = dataset['Close'].ewm(span=15, adjust=False).mean()\n",
        "\n",
        "  #dvel = dataset['Close']\n",
        "  #dv2 = np.array(dvel)\n",
        "  #dv3 = np.array(dvel)\n",
        "  #for i in range(1,len(dvel)):\n",
        "    #dv3[i] = dvel[i]-dvel[i-1]\n",
        "\n",
        "  #dfvel = pd.DataFrame(dv3)\n",
        "  #dataset['1st_Order'] = dfvel\n",
        "\n",
        "  #dvel = dataset['Close']\n",
        "  #da2 = np.array(dvel)\n",
        "  #for i in range(2,len(dvel)):\n",
        "    #da2[i] = dvel[i]-2*dvel[i-1]+dvel[i-2]\n",
        "\n",
        "  #dfacc=pd.DataFrame(da2)\n",
        "  #dataset['2nd_Order'] = dfacc\n",
        "\n",
        "  close = dataset.iloc[15:, 3:4]\n",
        "  dataset = dataset.drop(columns = 'Close', axis=1)\n",
        "  dataset = dataset.iloc[15:, :]\n",
        "  dataset['Close'] = close \n",
        "\n",
        "  #Removing Highly Co-related Data\n",
        "  dataset = dataset.drop(columns = ['High', 'Low'], axis = 1)\n",
        "  columns = ['Open', 'Volume', 'Range', 'a/d', '15MA', '15EMA', 'Close'] \n",
        "  for i in tqdm(columns) :\n",
        "     Q1 = dataset[i].quantile(0.25)\n",
        "     Q3 = dataset[i].quantile(0.75)\n",
        "     IQR = Q3-Q1\n",
        "     u = Q3 + (1.5*IQR)\n",
        "     l = Q1 - (1.5*IQR)\n",
        "     dataset = dataset.loc[(dataset[i]>l) & (dataset[i]<u),:]\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "MbH2ebWnTT2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DataNorm(dataset, normalizer_x, normalizer_y):\n",
        "  \n",
        "  X = dataset.iloc[:, 0:6]\n",
        "  Y = dataset.iloc[:, 6:]\n",
        "  #X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.35, random_state=42 , shuffle = False)\n",
        "  train_sz = int(X.shape[0]*0.65)\n",
        "  X_train,X_test = X.iloc[0:train_sz,:],X.iloc[train_sz:X.shape[0],:]\n",
        "  Y_train,Y_test = Y.iloc[0:train_sz],Y.iloc[train_sz:Y.shape[0]]\n",
        "\n",
        "  \n",
        "  \n",
        "  normalizedData_x = normalizer_x.fit_transform(X_train)\n",
        "  X_train = pd.DataFrame(normalizedData_x, index = X_train.index)\n",
        "  normX_test = normalizer_x.transform(X_test)\n",
        "  X_test = pd.DataFrame(normX_test, index = X_test.index)\n",
        " \n",
        "  normalizedData_y = normalizer_y.fit_transform(Y_train)\n",
        "  Y_train = pd.DataFrame(normalizedData_y, index = Y_train.index)\n",
        "  normY_test = normalizer_y.transform(Y_test)\n",
        "  Y_test = pd.DataFrame(normY_test, index = Y_test.index)\n",
        "  return X_train,X_test,Y_train,Y_test"
      ],
      "metadata": {
        "id": "t0AfiTJyX9a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Sequential Data ::\n",
        "def create_sequential_feed(X, y, seq_len):\n",
        "  tr_sequences, ts_sequences = [], []\n",
        "  for i in range(X.shape[0] - seq_len + 1):\n",
        "    tr_sequences.append(X.iloc[i:min(X.shape[0], i+seq_len)].values)\n",
        "    ts_sequences.append(y.iloc[min(y.shape[0], i+seq_len) - 1])\n",
        "  return np.array(tr_sequences), np.array(ts_sequences)"
      ],
      "metadata": {
        "id": "a4Bp6unYYxl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ModelTrain(X_train,X_test,Y_train,Y_test):\n",
        "  #Model Training ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "  #Create the Stacked LSTM model\n",
        "  \n",
        "  model=Sequential()\n",
        "  model.add(LSTM(250, activation = 'tanh', return_sequences=True,input_shape=(10,6)))\n",
        "  model.add(LSTM(250, activation = 'tanh', return_sequences=False, dropout = 0.3))\n",
        "  #model.add(LSTM(50, dropout = 0.2))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "  model.summary()\n",
        "\n",
        "  model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=20,batch_size=64,verbose=1)\n",
        "  train_predict=model.predict(X_train)\n",
        "  test_predict=model.predict(X_test)\n",
        "  return train_predict,test_predict"
      ],
      "metadata": {
        "id": "vd_SnPMHar0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Function for Training & Testing Data**"
      ],
      "metadata": {
        "id": "v6_fQxg3pFCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(ticker):\n",
        "  \n",
        "  dataset = collectData(ticker)\n",
        "  df = DataPreprocessing(dataset)\n",
        "  normalizer_x = MinMaxScaler(feature_range=(0,1))\n",
        "  normalizer_y = MinMaxScaler(feature_range=(0,1))\n",
        "  X_train,X_test,Y_train,Y_test = DataNorm(df,normalizer_x,normalizer_y)\n",
        "\n",
        "\n",
        "  #Creating Sequential Data  \n",
        "  X_train, Y_train = create_sequential_feed(X_train, Y_train, 10)\n",
        "  X_test, Y_test = create_sequential_feed(X_test, Y_test, 10)\n",
        "  \n",
        "  #Training the Model\n",
        "  train_predict,test_predict = ModelTrain(X_train,X_test,Y_train,Y_test)\n",
        "\n",
        "  #Results\n",
        "  Y_train = normalizer_y.inverse_transform(Y_train)\n",
        "  test_predict = normalizer_y.inverse_transform(test_predict)\n",
        "  Y_test = normalizer_y.inverse_transform(Y_test)\n",
        "  rmse_test = math.sqrt(mean_squared_error(Y_test,test_predict))\n",
        "  return  Y_train , test_predict,rmse_test\n",
        "  "
      ],
      "metadata": {
        "id": "-lfKUOtoQ3Bw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}