{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM[Open].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DBIh2Z7121E"
      },
      "source": [
        "In this notebook we'll implement Deep Learning based LSTM learning model for predcting stock's open prices .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnEzmW4N1x22",
        "outputId": "b6f7cd4e-4615-448f-9751-219669752177"
      },
      "source": [
        "!pip install yfinance "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.70)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUCNuPEBZSZy"
      },
      "source": [
        "#Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For reading stock data from yahoo\n",
        "from pandas_datareader.data import DataReader\n",
        "\n",
        "# For time stamps\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Collection::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "def collectData(ticker):\n",
        "  dataset = yf.download(ticker, start ='2015-01-01', end='2021-12-31', threads=True, progress = False)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "p322xxa-PBOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DataPreprocessing(dataset):\n",
        "  #Data Pre-processing:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "  \n",
        "  dataset = dataset.drop(columns = 'Adj Close', axis = 1)\n",
        "\n",
        "  #Deriving new features : \n",
        "  dataset['Range'] = dataset['High']-dataset['Low']\n",
        "  dataset['a/d'] = ((dataset['Close']-dataset['Low'])*(dataset['High']-dataset['Close']))/(dataset['High']-dataset['Low'])\n",
        "  dataset['15MA'] = dataset['Open'].rolling(window = 15).mean()\n",
        "  dataset['15EMA'] = dataset['Open'].ewm(span=15, adjust=False).mean()\n",
        "\n",
        "  #dvel = dataset['Close']\n",
        "  #dv2 = np.array(dvel)\n",
        "  #dv3 = np.array(dvel)\n",
        "  #for i in range(1,len(dvel)):\n",
        "    #dv3[i] = dvel[i]-dvel[i-1]\n",
        "\n",
        "  #dfvel = pd.DataFrame(dv3)\n",
        "  #dataset['1st_Order'] = dfvel\n",
        "\n",
        "  #dvel = dataset['Close']\n",
        "  #da2 = np.array(dvel)\n",
        "  #for i in range(2,len(dvel)):\n",
        "    #da2[i] = dvel[i]-2*dvel[i-1]+dvel[i-2]\n",
        "\n",
        "  #dfacc=pd.DataFrame(da2)\n",
        "  #dataset['2nd_Order'] = dfacc\n",
        "\n",
        "  open = dataset.iloc[15:, 0:1]\n",
        "  dataset = dataset.drop(columns = 'Open', axis=1)\n",
        "  dataset = dataset.iloc[15:, :]\n",
        "  dataset['Open'] = open \n",
        "\n",
        "  #Removing Highly Co-related Data\n",
        "  dataset = dataset.drop(columns = ['High', 'Low'], axis = 1)\n",
        "  columns = ['Close', 'Volume', 'Range', 'a/d', '15MA', '15EMA', 'Open'] \n",
        "  for i in tqdm(columns) :\n",
        "     Q1 = dataset[i].quantile(0.25)\n",
        "     Q3 = dataset[i].quantile(0.75)\n",
        "     IQR = Q3-Q1\n",
        "     u = Q3 + (1.5*IQR)\n",
        "     l = Q1 - (1.5*IQR)\n",
        "     dataset = dataset.loc[(dataset[i]>l) & (dataset[i]<u),:]\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "MbH2ebWnTT2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DataNorm(dataset, normalizer_x, normalizer_y):\n",
        "  \n",
        "  X = dataset.iloc[:, 0:6]\n",
        "  Y = dataset.iloc[:, 6:]\n",
        "  #X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.35, random_state=42 , shuffle = False)\n",
        "  train_sz = int(X.shape[0]*0.65)\n",
        "  X_train,X_test = X.iloc[0:train_sz,:],X.iloc[train_sz:X.shape[0],:]\n",
        "  Y_train,Y_test = Y.iloc[0:train_sz],Y.iloc[train_sz:Y.shape[0]]\n",
        "\n",
        "  \n",
        "  \n",
        "  normalizedData_x = normalizer_x.fit_transform(X_train)\n",
        "  X_train = pd.DataFrame(normalizedData_x, index = X_train.index)\n",
        "  normX_test = normalizer_x.transform(X_test)\n",
        "  X_test = pd.DataFrame(normX_test, index = X_test.index)\n",
        " \n",
        "  normalizedData_y = normalizer_y.fit_transform(Y_train)\n",
        "  Y_train = pd.DataFrame(normalizedData_y, index = Y_train.index)\n",
        "  normY_test = normalizer_y.transform(Y_test)\n",
        "  Y_test = pd.DataFrame(normY_test, index = Y_test.index)\n",
        "  return X_train,X_test,Y_train,Y_test"
      ],
      "metadata": {
        "id": "t0AfiTJyX9a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Sequential Data ::\n",
        "def create_sequential_feed(X, y, seq_len):\n",
        "  tr_sequences, ts_sequences = [], []\n",
        "  for i in range(X.shape[0] - seq_len + 1):\n",
        "    tr_sequences.append(X.iloc[i:min(X.shape[0], i+seq_len)].values)\n",
        "    ts_sequences.append(y.iloc[min(y.shape[0], i+seq_len) - 1])\n",
        "  return np.array(tr_sequences), np.array(ts_sequences)"
      ],
      "metadata": {
        "id": "a4Bp6unYYxl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ModelTrain(X_train,X_test,Y_train,Y_test):\n",
        "  #Model Training ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
        "  #Create the Stacked LSTM model\n",
        "  \n",
        "  model=Sequential()\n",
        "  model.add(LSTM(250, activation = 'tanh', return_sequences=True,input_shape=(10,6)))\n",
        "  model.add(LSTM(250, activation = 'tanh', return_sequences=False, dropout = 0.3))\n",
        "  #model.add(LSTM(50, dropout = 0.2))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "  model.summary()\n",
        "\n",
        "  model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=20,batch_size=64,verbose=1)\n",
        "  train_predict=model.predict(X_train)\n",
        "  test_predict=model.predict(X_test)\n",
        "  return train_predict,test_predict"
      ],
      "metadata": {
        "id": "vd_SnPMHar0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Function for Training & Testing Data**"
      ],
      "metadata": {
        "id": "v6_fQxg3pFCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(ticker):\n",
        "  \n",
        "  dataset = collectData(ticker)\n",
        "  df = DataPreprocessing(dataset)\n",
        "  normalizer_x = MinMaxScaler(feature_range=(0,1))\n",
        "  normalizer_y = MinMaxScaler(feature_range=(0,1))\n",
        "  X_train,X_test,Y_train,Y_test = DataNorm(df,normalizer_x,normalizer_y)\n",
        "\n",
        "\n",
        "  #Creating Sequential Data  \n",
        "  X_train, Y_train = create_sequential_feed(X_train, Y_train, 10)\n",
        "  X_test, Y_test = create_sequential_feed(X_test, Y_test, 10)\n",
        "  \n",
        "  #Training the Model\n",
        "  train_predict,test_predict = ModelTrain(X_train,X_test,Y_train,Y_test)\n",
        "\n",
        "  #Results\n",
        "  Y_train = normalizer_y.inverse_transform(Y_train)\n",
        "  test_predict = normalizer_y.inverse_transform(test_predict)\n",
        "  Y_test = normalizer_y.inverse_transform(Y_test)\n",
        "  rmse_test = math.sqrt(mean_squared_error(Y_test,test_predict))\n",
        "  return  Y_train , test_predict,rmse_test\n",
        "  "
      ],
      "metadata": {
        "id": "-lfKUOtoQ3Bw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}